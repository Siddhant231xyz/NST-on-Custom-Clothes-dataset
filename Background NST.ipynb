{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54da2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Function to replace background with a solid color\n",
    "def replace_background(image, new_bg_color=(255, 255, 255)):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.DEVICE = \"cpu\"  # Force CPU usage\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    outputs = predictor(image)\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    # Get the mask of the person\n",
    "    mask = outputs[\"instances\"].pred_masks[0].detach().cpu().numpy()\n",
    "\n",
    "    # Create a white background image with the same dimensions as input image\n",
    "    background = np.ones_like(image, dtype=np.uint8) * new_bg_color\n",
    "\n",
    "    # Bitwise-AND to get the foreground (person)\n",
    "    fg = cv2.bitwise_and(image, image, mask=mask.astype(np.uint8))\n",
    "\n",
    "    # Bitwise-AND to get the background\n",
    "    bg = cv2.bitwise_and(background, background, mask=(1 - mask).astype(np.uint8))\n",
    "\n",
    "    # Convert fg and bg to the same data type (np.uint8) before adding\n",
    "    fg = fg.astype(np.uint8)\n",
    "    bg = bg.astype(np.uint8)\n",
    "\n",
    "    # Combine foreground and background to get the final image\n",
    "    result = cv2.add(fg, bg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Read the input image\n",
    "image = cv2.imread(\"images/tiger.jpg\")\n",
    "\n",
    "# Check if image was properly loaded\n",
    "if image is None:\n",
    "    print(\"Error: Image not found or could not be opened.\")\n",
    "else:\n",
    "    # Replace background with white color\n",
    "    result_image = replace_background(image, new_bg_color=(1, 1, 1))\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Background Replaced Image\", result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(\"background_replaced_image.jpg\", result_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6984a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to replace background with a solid color\n",
    "def replace_background(image, new_bg_color=(255, 255, 255)):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.DEVICE = \"cpu\"  # Force CPU usage\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    outputs = predictor(image)\n",
    "    v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    # Get the mask of the person\n",
    "    mask = outputs[\"instances\"].pred_masks[0].detach().cpu().numpy()\n",
    "\n",
    "    # Create a white background image with the same dimensions as input image\n",
    "    background = np.ones_like(image, dtype=np.uint8) * new_bg_color\n",
    "\n",
    "    # Bitwise-AND to get the foreground (person)\n",
    "    fg = cv2.bitwise_and(image, image, mask=mask.astype(np.uint8))\n",
    "\n",
    "    # Bitwise-AND to get the background\n",
    "    bg = cv2.bitwise_and(background, background, mask=(1 - mask).astype(np.uint8))\n",
    "\n",
    "    # Convert fg and bg to the same data type (np.uint8) before adding\n",
    "    fg = fg.astype(np.uint8)\n",
    "    bg = bg.astype(np.uint8)\n",
    "\n",
    "    # Combine foreground and background to get the final image\n",
    "    result = cv2.add(fg, bg)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to preprocess the image for VGG19\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Function to deprocess the image for viewing\n",
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Function to compute content loss\n",
    "def content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "# Function to compute the gram matrix for style loss\n",
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)\n",
    "\n",
    "# Function to compute style loss\n",
    "def style_loss(base_style, gram_target):\n",
    "    batch_size, height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (height * width * channels)\n",
    "\n",
    "# Load the content and style images\n",
    "content_image = preprocess_image('./images/cat.jpeg')\n",
    "style_image = preprocess_image('./images/thescream.jpg')\n",
    "\n",
    "# Load the VGG19 model\n",
    "model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the layers to use for the style and content\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# Create the model for content and style\n",
    "outputs = [model.get_layer(name).output for name in style_layers + [content_layer]]\n",
    "style_model = Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "# Extract the style and content features\n",
    "style_outputs = style_model(style_image)\n",
    "content_outputs = style_model(content_image)\n",
    "\n",
    "# Compute the style features' Gram matrices\n",
    "style_grams = [gram_matrix(style_feature) for style_feature in style_outputs]\n",
    "\n",
    "# Set initial image for optimization\n",
    "initial_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
    "\n",
    "# Modify the style loss calculation in the optimization loop\n",
    "for i in range(1, 101):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = style_model(initial_image)\n",
    "        loss = sum([style_loss(output[j], style_grams[j]) for j in range(len(style_layers))])\n",
    "        loss += content_loss(output[-1], content_outputs[-1])\n",
    "    grad = tape.gradient(loss, initial_image)\n",
    "    optimizer.apply_gradients([(grad, initial_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i} completed')\n",
    "\n",
    "# Convert final image to uint8\n",
    "final_image = deprocess_image(initial_image.numpy()[0])\n",
    "\n",
    "# Read the input image for replacing background\n",
    "image = cv2.imread(\"images/cat.jpeg\")\n",
    "\n",
    "# Check if image was properly loaded\n",
    "if image is None:\n",
    "    print(\"Error: Image not found or could not be opened.\")\n",
    "else:\n",
    "    # Replace background with white color\n",
    "    result_image = replace_background(image, new_bg_color=(255, 255, 255))\n",
    "\n",
    "    # Resize final image to match the replaced background\n",
    "    final_image_resized = cv2.resize(final_image, (result_image.shape[1], result_image.shape[0]))\n",
    "\n",
    "    # Convert final image to BGR for OpenCV\n",
    "    final_image_bgr = final_image_resized[:, :, ::-1]\n",
    "\n",
    "    # Create a mask for the background\n",
    "    background_mask = np.all(result_image == [255, 255, 255], axis=-1)\n",
    "\n",
    "    # Apply style transfer only to the background\n",
    "    for i in range(3):  # Process each channel separately\n",
    "        final_image_bgr[..., i] = np.where(background_mask, final_image_bgr[..., i], result_image[..., i])\n",
    "\n",
    "    # Convert back to RGB\n",
    "    final_image_rgb = final_image_bgr[:, :, ::-1]\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Final Result\", final_image_rgb)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(\"final_result.jpg\", final_image_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365e30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 completed\n",
      "No instances detected.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 123>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m background \u001b[38;5;241m=\u001b[39m replace_background(image, new_bg_color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m))\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Convert background to uint8\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m background_uint8 \u001b[38;5;241m=\u001b[39m \u001b[43mbackground\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Resize final image to match the replaced background\u001b[39;00m\n\u001b[0;32m    133\u001b[0m final_image_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(final_image, (background\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], background\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to replace background with a solid color\n",
    "def replace_background(image, new_bg_color=(255, 255, 255)):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.DEVICE = \"cpu\"  # Force CPU usage\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    outputs = predictor(image)\n",
    "    instances = outputs[\"instances\"]\n",
    "\n",
    "    # Check if instances are detected\n",
    "    if instances is None or len(instances) == 0:\n",
    "        print(\"No instances detected.\")\n",
    "        return None\n",
    "\n",
    "    # Get the mask of the person\n",
    "    mask = instances.pred_masks[0].detach().cpu().numpy()\n",
    "\n",
    "    # Create a white background image with the same dimensions as input image\n",
    "    background = np.ones_like(image, dtype=np.uint8) * new_bg_color\n",
    "\n",
    "    # Bitwise-AND to get the background\n",
    "    bg = cv2.bitwise_and(background, background, mask=(1 - mask).astype(np.uint8))\n",
    "\n",
    "    return bg\n",
    "\n",
    "\n",
    "# Function to preprocess the image for VGG19\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Function to deprocess the image for viewing\n",
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Function to compute content loss\n",
    "def content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "# Function to compute the gram matrix for style loss\n",
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)\n",
    "\n",
    "# Function to compute style loss\n",
    "def style_loss(base_style, gram_target):\n",
    "    batch_size, height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (height * width * channels)\n",
    "\n",
    "# Load the content and style images\n",
    "content_image = preprocess_image('./images/shirt.jpeg')\n",
    "style_image = preprocess_image('./images/starrynight.jpg')\n",
    "\n",
    "# Load the VGG19 model\n",
    "model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the layers to use for the style and content\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# Create the model for content and style\n",
    "outputs = [model.get_layer(name).output for name in style_layers + [content_layer]]\n",
    "style_model = Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "# Extract the style and content features\n",
    "style_outputs = style_model(style_image)\n",
    "content_outputs = style_model(content_image)\n",
    "\n",
    "# Compute the style features' Gram matrices\n",
    "style_grams = [gram_matrix(style_feature) for style_feature in style_outputs]\n",
    "\n",
    "# Set initial image for optimization\n",
    "initial_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
    "\n",
    "# Modify the style loss calculation in the optimization loop\n",
    "for i in range(1, 101):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = style_model(initial_image)\n",
    "        loss = sum([style_loss(output[j], style_grams[j]) for j in range(len(style_layers))])\n",
    "        loss += content_loss(output[-1], content_outputs[-1])\n",
    "    grad = tape.gradient(loss, initial_image)\n",
    "    optimizer.apply_gradients([(grad, initial_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i} completed')\n",
    "\n",
    "# Convert final image to uint8\n",
    "final_image = deprocess_image(initial_image.numpy()[0])\n",
    "\n",
    "# Read the input image for replacing background\n",
    "image = cv2.imread(\"images/shirt.jpeg\")\n",
    "\n",
    "# Check if image was properly loaded\n",
    "if image is None:\n",
    "    print(\"Error: Image not found or could not be opened.\")\n",
    "else:\n",
    "    # Replace background with white color\n",
    "    background = replace_background(image, new_bg_color=(255, 255, 255))\n",
    "    \n",
    "    # Convert background to uint8\n",
    "    background_uint8 = background.astype(np.uint8)\n",
    "\n",
    "    # Resize final image to match the replaced background\n",
    "    final_image_resized = cv2.resize(final_image, (background.shape[1], background.shape[0]))\n",
    "\n",
    "    # Convert final image to BGR for OpenCV\n",
    "    final_image_bgr = final_image_resized[:, :, ::-1]\n",
    "\n",
    "    # Overlay styled content on replaced background\n",
    "    final_result = cv2.add(final_image_bgr, background_uint8)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Final Result\", final_result)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(\"final_result.jpg\", final_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d7a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Save the trained model\n",
    "output_model_path = \"C://Users//admin//Desktop//Neural Style Transfers//output//trained_model.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae78e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 completed\n",
      "Available fields in instances object: dict_keys(['pred_boxes', 'scores', 'pred_classes', 'pred_masks'])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sample_image_path = \"./images/shirt.jpeg\"\n",
    "\n",
    "sample_image_path = \"C://Users//admin//Desktop//New folder//images//0d293c69-ba4f-4ffc-b6b9-fa05316a02c5.jpg\"\n",
    "sample_image = cv2.imread(sample_image_path)\n",
    "\n",
    "# Function to replace background with a solid color\n",
    "def replace_background(image, new_bg_color=(255, 255, 255)):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.WEIGHTS = output_model_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    outputs = predictor(image)\n",
    "    instances = outputs[\"instances\"]\n",
    "\n",
    "    # Check if instances are detected\n",
    "    if instances is None or len(instances) == 0:\n",
    "        print(\"No instances detected.\")\n",
    "        return None\n",
    "\n",
    "    # Extract the segmentation masks\n",
    "    masks = instances.pred_masks.numpy()\n",
    "\n",
    "    # Apply the masks to the background\n",
    "    for mask in masks:\n",
    "        mask = mask.astype(np.uint8)\n",
    "        # Create a copy of the background for blending\n",
    "        blended_image = background.copy()\n",
    "        # Blend the masked image with the background using numpy operations\n",
    "        blended_image[mask == 1] = (sample_image[mask == 1] * 0.5 + background[mask == 1] * 0.5).astype(np.uint8)\n",
    "        # Update the background with the blended image\n",
    "        background = blended_image\n",
    "\n",
    "    # # Replace the background with white\n",
    "    # background_color = (255, 255, 255)  # White color\n",
    "    # background = np.ones(sample_image.shape, dtype=np.uint8) * new_bg_color  \n",
    "\n",
    "    # # Create a white background image with the same dimensions as input image\n",
    "    # background = np.ones_like(image, dtype=np.uint8) * new_bg_color\n",
    "\n",
    "    # Bitwise-AND to get the background\n",
    "    bg = cv2.bitwise_and(background, background, mask=(1 - mask).astype(np.uint8))\n",
    "\n",
    "    return bg\n",
    "\n",
    "\n",
    "# Function to preprocess the image for VGG19\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Function to deprocess the image for viewing\n",
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Function to compute content loss\n",
    "def content_loss(base_content, target):\n",
    "    return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "# Function to compute the gram matrix for style loss\n",
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)\n",
    "\n",
    "# Function to compute style loss\n",
    "def style_loss(base_style, gram_target):\n",
    "    batch_size, height, width, channels = base_style.get_shape().as_list()\n",
    "    gram_style = gram_matrix(base_style)\n",
    "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (height * width * channels)\n",
    "\n",
    "# Load the content and style images\n",
    "content_image = preprocess_image('C://Users//admin//Desktop//New folder//images//0d293c69-ba4f-4ffc-b6b9-fa05316a02c5.jpg')\n",
    "#content_image = preprocess_image(\"./images/shirt.jpeg\")\n",
    "style_image = preprocess_image('./images/starrynight.jpg')\n",
    "\n",
    "# Load the VGG19 model\n",
    "model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the layers to use for the style and content\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# Create the model for content and style\n",
    "outputs = [model.get_layer(name).output for name in style_layers + [content_layer]]\n",
    "style_model = Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "# Extract the style and content features\n",
    "style_outputs = style_model(style_image)\n",
    "content_outputs = style_model(content_image)\n",
    "\n",
    "# Compute the style features' Gram matrices\n",
    "style_grams = [gram_matrix(style_feature) for style_feature in style_outputs]\n",
    "\n",
    "# Set initial image for optimization\n",
    "initial_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
    "\n",
    "# Modify the style loss calculation in the optimization loop\n",
    "for i in range(1, 101):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = style_model(initial_image)\n",
    "        loss = sum([style_loss(output[j], style_grams[j]) for j in range(len(style_layers))])\n",
    "        loss += content_loss(output[-1], content_outputs[-1])\n",
    "    grad = tape.gradient(loss, initial_image)\n",
    "    optimizer.apply_gradients([(grad, initial_image)])\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i} completed')\n",
    "\n",
    "# Convert final image to uint8\n",
    "final_image = deprocess_image(initial_image.numpy()[0])\n",
    "\n",
    "# Read the input image for replacing background\n",
    "image = cv2.imread('C://Users//admin//Desktop//New folder//images//0d293c69-ba4f-4ffc-b6b9-fa05316a02c5.jpg')\n",
    "#image = cv2.imread('./images/shirt.jpeg')\n",
    "# Check if image was properly loaded\n",
    "if image is None:\n",
    "    print(\"Error: Image not found or could not be opened.\")\n",
    "else:\n",
    "    # # Replace background with white color\n",
    "    # background = replace_background(sample_image, new_bg_color=(255, 255, 255))\n",
    "\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.DATASETS.TRAIN = (\"shirt_dataset_train\",)\n",
    "    cfg.DATASETS.TEST = ()  # No test dataset\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = output_model_path\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "    cfg.SOLVER.BASE_LR = 0.0025\n",
    "    cfg.SOLVER.MAX_ITER = 50  # You can adjust this\n",
    "    cfg.SOLVER.STEPS = []  # You can add learning rate steps here\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # You can adjust this\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class (Shirt)\n",
    "\n",
    "    # Enable mask prediction\n",
    "    cfg.MODEL.MASK_ON = True\n",
    "\n",
    "\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for this model\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = predictor(sample_image)\n",
    "\n",
    "    # Get the predicted instances\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "    # Print the available fields\n",
    "    print(\"Available fields in instances object:\", instances.get_fields().keys())\n",
    "\n",
    "    # Check if 'pred_masks' is available\n",
    "    if \"pred_masks\" not in instances.get_fields():\n",
    "        print(\"Model does not predict masks!\")\n",
    "    else:\n",
    "        # Extract the segmentation masks\n",
    "        masks = instances.pred_masks.numpy()\n",
    "\n",
    "        # Replace the background with white\n",
    "        background_color = (255, 255, 255)  # White color\n",
    "        background = np.ones(sample_image.shape, dtype=np.uint8) * background_color\n",
    "\n",
    "        # Apply the masks to the background\n",
    "        for mask in masks:\n",
    "            mask = mask.astype(np.uint8)\n",
    "            # Create a copy of the background for blending\n",
    "            blended_image = background.copy()\n",
    "            # Blend the masked image with the background using numpy operations\n",
    "            blended_image[mask == 1] = (sample_image[mask == 1] * 0.5 + background[mask == 1] * 0.5).astype(np.uint8)\n",
    "            # Update the background with the blended image\n",
    "            background = blended_image\n",
    "\n",
    "        # Save the resulting image\n",
    "        output_image_path = \"output_image3.jpg\"\n",
    "        cv2.imwrite(output_image_path, background)\n",
    "    \n",
    "    # # Convert background to uint8\n",
    "    background_uint8 = background.astype(np.uint8)\n",
    "\n",
    "    # Resize final image to match the replaced background\n",
    "    final_image_resized = cv2.resize(final_image, (background.shape[1], background.shape[0]))\n",
    "\n",
    "    # Convert final image to BGR for OpenCV\n",
    "    final_image_bgr = final_image_resized[:, :, ::-1]\n",
    "\n",
    "    # Overlay styled content on replaced background\n",
    "    final_result = cv2.add(final_image_bgr, background_uint8)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Original Image\", image)\n",
    "    cv2.imshow(\"Final Result\", final_result)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(\"final_result3.jpg\", final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a6e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Load a sample image\n",
    "# sample_image_path = \"C://Users//admin//Desktop//New folder//images//00e745c9-97d9-429d-8c3f-d3db7a2d2991.jpg\"\n",
    "sample_image_path = \"./images/shirt.jpeg\"\n",
    "sample_image = cv2.imread(sample_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb12cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Save the trained model\n",
    "output_model_path = \"C://Users//admin//Desktop//Neural Style Transfers//output//trained_model.pth\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "outputs = predictor(sample_image)\n",
    "\n",
    "# Get the predicted instances\n",
    "instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "# Print the available fields\n",
    "print(\"Available fields in instances object:\", instances.get_fields().keys())\n",
    "\n",
    "# Check if 'pred_masks' is available\n",
    "if \"pred_masks\" not in instances.get_fields():\n",
    "    print(\"Model does not predict masks!\")\n",
    "else:\n",
    "    # Extract the segmentation masks\n",
    "    masks = instances.pred_masks.numpy()\n",
    "\n",
    "    # Replace the background with white\n",
    "    background_color = (255, 255, 255)  # White color\n",
    "    background = np.ones(sample_image.shape, dtype=np.uint8) * background_color\n",
    "\n",
    "    # Apply the masks to the background\n",
    "    for mask in masks:\n",
    "        mask = mask.astype(np.uint8)\n",
    "        # Create a copy of the background for blending\n",
    "        blended_image = background.copy()\n",
    "        # Blend the masked image with the background using numpy operations\n",
    "        blended_image[mask == 1] = (sample_image[mask == 1] * 0.5 + background[mask == 1] * 0.5).astype(np.uint8)\n",
    "        # Update the background with the blended image\n",
    "        background = blended_image\n",
    "\n",
    "    # Save the resulting image\n",
    "    output_image_path = \"output_image.jpg\"\n",
    "    cv2.imwrite(output_image_path, background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be176fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Desktop\\Neural Style Transfers\\detectron2\\layers\\wrappers.py:127: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  x = F.conv2d(\n",
      "c:\\Users\\admin\\Desktop\\Neural Style Transfers\\detectron2\\layers\\wrappers.py:127: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  x = F.conv2d(\n",
      "c:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available fields in instances object: dict_keys(['pred_boxes', 'scores', 'pred_classes', 'pred_masks'])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"det/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"shirt_dataset_train\",)\n",
    "cfg.DATASETS.TEST = ()  # No test dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = output_model_path\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.0025\n",
    "cfg.SOLVER.MAX_ITER = 50  # You can adjust this\n",
    "cfg.SOLVER.STEPS = []  # You can add learning rate steps here\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # You can adjust this\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class (Shirt)\n",
    "\n",
    "# Enable mask prediction\n",
    "cfg.MODEL.MASK_ON = True\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for this model\n",
    "predictor = DefaultPredictor(cfg)\n",
    "# Perform inference\n",
    "outputs = predictor(sample_image)\n",
    "\n",
    "# Get the predicted instances\n",
    "instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "# Print the available fields\n",
    "print(\"Available fields in instances object:\", instances.get_fields().keys())\n",
    "\n",
    "# Check if 'pred_masks' is available\n",
    "if \"pred_masks\" not in instances.get_fields():\n",
    "    print(\"Model does not predict masks!\")\n",
    "else:\n",
    "    # Extract the segmentation masks\n",
    "    masks = instances.pred_masks.numpy()\n",
    "\n",
    "    # Replace the background with white\n",
    "    background_color = (255, 255, 255)  # White color\n",
    "    background = np.ones(sample_image.shape, dtype=np.uint8) * background_color\n",
    "\n",
    "    # Apply the masks to the background\n",
    "    for mask in masks:\n",
    "        mask = mask.astype(np.uint8)\n",
    "        # Create a copy of the background for blending\n",
    "        blended_image = background.copy()\n",
    "        # Blend the masked image with the background using numpy operations\n",
    "        blended_image[mask == 1] = (sample_image[mask == 1] * 0.5 + background[mask == 1] * 0.5).astype(np.uint8)\n",
    "        # Update the background with the blended image\n",
    "        background = blended_image\n",
    "\n",
    "    # Save the resulting image\n",
    "    output_image_path = \"output_image1.jpg\"\n",
    "    cv2.imwrite(output_image_path, background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7257de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
